{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89b1b56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
      "     |################################| 3.4 MB 64.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from transformers) (1.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (20.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.1)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
      "     |################################| 61 kB 313 kB/s             \n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "     |################################| 895 kB 89.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2017.4.5)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "     |################################| 3.3 MB 84.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.43.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from transformers) (5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.1)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "     |################################| 40 kB 3.2 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.6/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->transformers) (2.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.14.0)\n",
      "Collecting click\n",
      "  Downloading click-8.0.3-py3-none-any.whl (97 kB)\n",
      "     |################################| 97 kB 4.1 MB/s             \n",
      "\u001b[?25hInstalling collected packages: packaging, click, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "\u001b[33m  WARNING: The script sacremoses is installed in '/home/sharmarohit/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/sharmarohit/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script transformers-cli is installed in '/home/sharmarohit/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed click-8.0.3 huggingface-hub-0.2.1 packaging-21.3 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n",
      "     |################################| 78 kB 3.7 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in ./.local/lib/python3.6/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: tokenizers>=0.10.3 in ./.local/lib/python3.6/site-packages (from sentence-transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.43.0)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.10.1-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "     |################################| 881.9 MB 10 kB/s               MB/s eta 0:00:05MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.4.5)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.1.85)\n",
      "Requirement already satisfied: huggingface-hub in ./.local/lib/python3.6/site-packages (from sentence-transformers) (0.2.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (4.0.1)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.3)\n",
      "Requirement already satisfied: sacremoses in ./.local/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.46)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2017.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.14.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.11.2-cp36-cp36m-manylinux1_x86_64.whl (23.3 MB)\n",
      "     |################################| 23.3 MB 82.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.6/dist-packages (from torchvision->sentence-transformers) (6.2.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.6/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (8.0.3)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=120999 sha256=4adae87fb56559cc4099e438af837a50d5ef56f9f59769e2fd6f5bb1e693ba26\n",
      "  Stored in directory: /home/sharmarohit/.cache/pip/wheels/4e/6f/20/06e0c1e209742a37ce7a5a9aa4e420a3abd5081c65b4b34d0a\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: torch, torchvision, sentence-transformers\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/home/sharmarohit/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed sentence-transformers-2.1.0 torch-1.10.1 torchvision-0.11.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import torch\n",
    "import io\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "# from transformers import *\n",
    "# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "#!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#!pip install sentencepiece\n",
    "\n",
    "##Set random values\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "170d8512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length_query =  64 seed_times_size =  1 batch_size_train_value =  1.0 top_k_bert_value =  5 flag_Bert_MLM =  True\n",
      "Start Writing One-Walk at : 1641444787.633507\n",
      "End Writing One-Walk at : 1641444787.635887\n",
      "Start converting text data to numbers for input to IRL : 1641444787.638229\n",
      "(10904, 64)\n",
      "(5452, 64)\n",
      "End converting text data to numbers for input to IRL : 1641444841.9937284\n",
      "vocab_size =  9452\n",
      "Final Batch Size =  64\n",
      "1\n",
      "2\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/sharmarohit/rewarder.py:92: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "2022-01-06 04:54:04.263167: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2022-01-06 04:54:04.319277: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2693670000 Hz\n",
      "2022-01-06 04:54:04.333526: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x910c9c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-01-06 04:54:04.333624: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-01-06 04:54:04.338433: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2022-01-06 04:54:04.695181: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x9126c30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-01-06 04:54:04.695255: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2022-01-06 04:54:04.705259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 1797:00:00.0\n",
      "2022-01-06 04:54:04.705874: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2022-01-06 04:54:04.707828: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2022-01-06 04:54:04.709474: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2022-01-06 04:54:04.710384: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2022-01-06 04:54:04.712668: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2022-01-06 04:54:04.714355: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2022-01-06 04:54:04.719000: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-01-06 04:54:04.726965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
      "2022-01-06 04:54:04.727060: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2022-01-06 04:54:04.730685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-01-06 04:54:04.730728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
      "2022-01-06 04:54:04.730743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
      "2022-01-06 04:54:04.738730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 29049 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 1797:00:00.0, compute capability: 7.0)\n",
      "Start pre-training...\n",
      "2022-01-06 04:54:06.555943: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "pre-train epoch  0 test_loss  1.1055006\n",
      "Start pre-training rewarder...\n",
      "Reward pre_training Speed:183.113\n",
      "#########################################################################\n",
      "Start Adversarial Training...\n",
      "output done for batch --   0\n",
      "MaxentPolicy Gradient 0 round, Speed:562.193, Loss:-15.571\n",
      "0.9278683960004601  avg_train_loss_g\n",
      "Reward training 0 round, Speed:825.524, Loss:-8.231\n",
      "output done for batch --   1\n",
      "MaxentPolicy Gradient 1 round, Speed:560.530, Loss:-18.486\n",
      "2.6710159307302432  avg_train_loss_g\n",
      "Reward training 1 round, Speed:811.447, Loss:-11.317\n",
      "output done for batch --   2\n",
      "MaxentPolicy Gradient 2 round, Speed:549.061, Loss:-19.816\n",
      "3.8591271361639334  avg_train_loss_g\n",
      "Reward training 2 round, Speed:809.413, Loss:-13.572\n",
      "output done for batch --   3\n",
      "MaxentPolicy Gradient 3 round, Speed:550.514, Loss:-20.717\n",
      "4.813533630481986  avg_train_loss_g\n",
      "Reward training 3 round, Speed:809.509, Loss:-19.105\n",
      "output done for batch --   4\n",
      "MaxentPolicy Gradient 4 round, Speed:549.723, Loss:-21.491\n",
      "5.390652091004128  avg_train_loss_g\n",
      "Reward training 4 round, Speed:809.184, Loss:-23.378\n",
      "output done for batch --   5\n",
      "MaxentPolicy Gradient 5 round, Speed:547.844, Loss:-21.979\n",
      "5.808133402536082  avg_train_loss_g\n",
      "Reward training 5 round, Speed:809.681, Loss:-24.311\n",
      "output done for batch --   6\n",
      "MaxentPolicy Gradient 6 round, Speed:549.063, Loss:-22.446\n",
      "6.215104973593424  avg_train_loss_g\n",
      "Reward training 6 round, Speed:809.438, Loss:-27.712\n",
      "output done for batch --   7\n",
      "MaxentPolicy Gradient 7 round, Speed:549.974, Loss:-22.764\n",
      "6.482169922008071  avg_train_loss_g\n",
      "Reward training 7 round, Speed:804.738, Loss:-30.494\n",
      "output done for batch --   8\n",
      "MaxentPolicy Gradient 8 round, Speed:544.120, Loss:-23.147\n",
      "6.645320232524428  avg_train_loss_g\n",
      "Reward training 8 round, Speed:800.594, Loss:-32.547\n",
      "output done for batch --   9\n",
      "MaxentPolicy Gradient 9 round, Speed:546.888, Loss:-23.217\n",
      "6.771352485168812  avg_train_loss_g\n",
      "Reward training 9 round, Speed:803.629, Loss:-33.422\n",
      "output done for batch --   10\n",
      "MaxentPolicy Gradient 10 round, Speed:550.976, Loss:-23.231\n",
      "6.869882278664168  avg_train_loss_g\n",
      "Reward training 10 round, Speed:804.734, Loss:-35.933\n",
      "output done for batch --   11\n",
      "MaxentPolicy Gradient 11 round, Speed:553.455, Loss:-23.680\n",
      "6.982091903686523  avg_train_loss_g\n",
      "Reward training 11 round, Speed:807.181, Loss:-41.081\n",
      "output done for batch --   12\n",
      "MaxentPolicy Gradient 12 round, Speed:547.001, Loss:-24.736\n",
      "7.580488942390264  avg_train_loss_g\n",
      "Reward training 12 round, Speed:807.300, Loss:-43.961\n",
      "output done for batch --   13\n",
      "MaxentPolicy Gradient 13 round, Speed:550.877, Loss:-25.415\n",
      "7.8472950680311335  avg_train_loss_g\n",
      "Reward training 13 round, Speed:809.538, Loss:-46.984\n",
      "output done for batch --   14\n",
      "MaxentPolicy Gradient 14 round, Speed:553.377, Loss:-25.480\n",
      "7.6574203358140105  avg_train_loss_g\n",
      "Reward training 14 round, Speed:811.350, Loss:-48.911\n",
      "output done for batch --   15\n",
      "MaxentPolicy Gradient 15 round, Speed:548.601, Loss:-26.373\n",
      "7.69205196513686  avg_train_loss_g\n",
      "Reward training 15 round, Speed:806.039, Loss:-50.495\n",
      "output done for batch --   16\n",
      "MaxentPolicy Gradient 16 round, Speed:544.186, Loss:-27.230\n",
      "7.8228857129119165  avg_train_loss_g\n",
      "Reward training 16 round, Speed:804.802, Loss:-51.770\n",
      "output done for batch --   17\n",
      "MaxentPolicy Gradient 17 round, Speed:543.256, Loss:-27.740\n",
      "7.703514853189158  avg_train_loss_g\n",
      "Reward training 17 round, Speed:804.776, Loss:-52.310\n",
      "output done for batch --   18\n",
      "MaxentPolicy Gradient 18 round, Speed:543.431, Loss:-29.053\n",
      "7.875063374985096  avg_train_loss_g\n",
      "Reward training 18 round, Speed:804.114, Loss:-52.977\n",
      "output done for batch --   19\n",
      "MaxentPolicy Gradient 19 round, Speed:549.341, Loss:-29.665\n",
      "7.604293518288191  avg_train_loss_g\n",
      "Reward training 19 round, Speed:805.318, Loss:-53.092\n",
      "output done for batch --   20\n",
      "MaxentPolicy Gradient 20 round, Speed:543.336, Loss:-30.221\n",
      "7.617760902227357  avg_train_loss_g\n",
      "Reward training 20 round, Speed:804.627, Loss:-53.681\n",
      "output done for batch --   21\n",
      "MaxentPolicy Gradient 21 round, Speed:549.063, Loss:-31.346\n",
      "8.171151161193848  avg_train_loss_g\n",
      "Reward training 21 round, Speed:806.309, Loss:-53.405\n",
      "output done for batch --   22\n",
      "MaxentPolicy Gradient 22 round, Speed:544.304, Loss:-32.977\n",
      "8.23621065117592  avg_train_loss_g\n",
      "Reward training 22 round, Speed:805.802, Loss:-53.622\n",
      "output done for batch --   23\n",
      "MaxentPolicy Gradient 23 round, Speed:544.624, Loss:-34.226\n",
      "8.501463823540266  avg_train_loss_g\n",
      "Reward training 23 round, Speed:805.983, Loss:-53.907\n",
      "output done for batch --   24\n",
      "MaxentPolicy Gradient 24 round, Speed:549.372, Loss:-35.091\n",
      "8.77522785719051  avg_train_loss_g\n",
      "Reward training 24 round, Speed:806.780, Loss:-53.939\n",
      "output done for batch --   25\n",
      "MaxentPolicy Gradient 25 round, Speed:545.396, Loss:-35.309\n",
      "8.662794224051542  avg_train_loss_g\n",
      "Reward training 25 round, Speed:808.528, Loss:-53.938\n",
      "output done for batch --   26\n",
      "MaxentPolicy Gradient 26 round, Speed:546.185, Loss:-35.852\n",
      "8.976185255272444  avg_train_loss_g\n",
      "Reward training 26 round, Speed:809.255, Loss:-53.965\n",
      "output done for batch --   27\n",
      "MaxentPolicy Gradient 27 round, Speed:549.652, Loss:-35.875\n",
      "9.020731005557748  avg_train_loss_g\n",
      "Reward training 27 round, Speed:805.588, Loss:-53.938\n",
      "output done for batch --   28\n",
      "MaxentPolicy Gradient 28 round, Speed:547.342, Loss:-36.593\n",
      "9.204374812370123  avg_train_loss_g\n",
      "Reward training 28 round, Speed:804.705, Loss:-53.942\n",
      "output done for batch --   29\n",
      "MaxentPolicy Gradient 29 round, Speed:548.225, Loss:-36.760\n",
      "9.436012378958768  avg_train_loss_g\n",
      "Reward training 29 round, Speed:805.929, Loss:-53.811\n"
     ]
    }
   ],
   "source": [
    "!python irl_seqGAN.py --seed_file \"save/seed.txt\" --one_walk_file \"save/one_walk.txt\" --BERT_MLM_flag False --pre_train_steps 10 --size_seed_times 1 --batch_size_train 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cea0620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
